{
    "nbformat_minor": 1, 
    "metadata": {
        "language_info": {
            "pygments_lexer": "ipython2", 
            "name": "python", 
            "version": "2.7.11", 
            "file_extension": ".py", 
            "nbconvert_exporter": "python", 
            "mimetype": "text/x-python", 
            "codemirror_mode": {
                "name": "ipython", 
                "version": 2
            }
        }, 
        "kernelspec": {
            "language": "python", 
            "name": "python2-spark21", 
            "display_name": "Python 2 with Spark 2.1"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "source": "# If required, use the following code snippets to workaround the \"Insert to Code\" bug in DSX Local 1.1.2\n### There are 3 code cells below:\n    * The first two are code snippets for the two datasets used in the Python notebook\n    * The third is the code snippet for the dataset used by the Scala notebook", 
            "metadata": {
                "collapsed": true
            }, 
            "cell_type": "markdown"
        }, 
        {
            "execution_count": null, 
            "metadata": {}, 
            "source": "# For Python visualization notebook: Automatically generated code for inserting Pandas dataframe for \"churn_rate_visualization\" dataset\nimport dsx_core_utils\nimport requests\nimport jaydebeapi\nfrom pyspark.sql import SparkSession\nimport os\nimport pandas as pd\n# Add asset from remote connection\ndf3 = None\ndataSet = dsx_core_utils.get_remote_data_set_info('churn_rate_visualization')\ndataSource = dsx_core_utils.get_data_source_info(dataSet['datasource'])\n#Differentiate between hdfs and jdbc\nif (dataSource['type'] == \"HDFS\"):\n\t#Use case for RPC port with hdfs protocol\n\tif (dataSource['URL'] == \"\"):\n\t\turl = 'hdfs://' + dataSource['host'] + ':' + str(dataSource['port'])\n\t\tpath = dataSet['file']\n\t\tfile_fullpath = url + path\n\t\tsparkSession = SparkSession(sc).builder.getOrCreate()\n\t\tdf3 = sparkSession.read.csv(file_fullpath, header = \"true\", inferSchema = \"true\")\n\t\tdf3 = df3.toPandas()\n\t#Use case for HTTP Port with a webhdfs URL\n\telse:\n\t\tif (dataSource['URL'].endswith('/')):\n\t\t\turl = dataSource['URL'][:-1] + dataSet['file'] + \"?op=OPEN\"\n\t\telse:\n\t\t\turl = dataSource['URL'] + dataSet['file'] + \"?op=OPEN\"\n\t\theaders = {\"Authorization\": os.environ.get('DSX_TOKEN')}\n\t\tresponse = requests.request(\"GET\", url, headers=headers, timeout=10, verify=False, allow_redirects=True)\n\t\tif response.status_code != 200:\n\t\t\traise Exception(\"get_data_source_info: \" + str(response.status_code) + \" returned when sending a request to \\\"\" + url+\"\\\"\")\n\t\telse:\n\t\t\text = \"\"\n\t\t\tif (dataSet['file'].endswith('csv')):\n\t\t\t\text = \".csv\"\n\t\t\telif (dataSet['file'].endswith('txt')):\n\t\t\t\text = \".txt\"\n\t\t\telse:\n\t\t\t\traise Exception(\"Invalid file type that is not txt or csv\")\n\t\t\tcsvFile = open(\"output\" + ext,\"w+\")\n\t\t\tcsvFile.write(response.text)\n\t\t\tcsvFile.close()\n\t\t\tdf3 = pd.read_csv(\"output\" + ext)\n\t\t\t# Files output.csv/output.txt/output.pkl will persist\n\t\t\tdf3.to_pickle('output.pkl')\n\t\t\tdf3 = pd.read_pickle('output.pkl')\nelse:\n\tconn = jaydebeapi.connect(dataSource['driver_class'], [dataSource['URL'], dataSource['user'], dataSource['password']])\n\tif (len(dataSet['schema'].strip()) != 0):\n\t\tdf3 = pd.read_sql('select * from ' + dataSet['schema'] + '.' + dataSet['table'], con=conn)\n\telse:\n\t\tdf3 = pd.read_sql('select * from ' + dataSet['table'], con=conn)\ndf3.head()\n", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {}, 
            "source": "# For Python visualization notebook:  Automatically generated code for inserting Pandas dataframe for \"cust_summary_visualization\" dataset\n\ndf4 = None\ndataSet = dsx_core_utils.get_remote_data_set_info('cust_summary_visualization')\ndataSource = dsx_core_utils.get_data_source_info(dataSet['datasource'])\n#Differentiate between hdfs and jdbc\nif (dataSource['type'] == \"HDFS\"):\n\t#Use case for RPC port with hdfs protocol\n\tif (dataSource['URL'] == \"\"):\n\t\turl = 'hdfs://' + dataSource['host'] + ':' + str(dataSource['port'])\n\t\tpath = dataSet['file']\n\t\tfile_fullpath = url + path\n\t\tsparkSession = SparkSession(sc).builder.getOrCreate()\n\t\tdf4 = sparkSession.read.csv(file_fullpath, header = \"true\", inferSchema = \"true\")\n\t\tdf4 = df4.toPandas()\n\t#Use case for HTTP Port with a webhdfs URL\n\telse:\n\t\tif (dataSource['URL'].endswith('/')):\n\t\t\turl = dataSource['URL'][:-1] + dataSet['file'] + \"?op=OPEN\"\n\t\telse:\n\t\t\turl = dataSource['URL'] + dataSet['file'] + \"?op=OPEN\"\n\t\theaders = {'Authorization': os.environ.get('DSX_TOKEN')}\n\t\tresponse = requests.request(\"GET\", url, headers=headers, timeout=10, verify=False, allow_redirects=True)\n\t\tif response.status_code != 200:\n\t\t\traise Exception(\"get_data_source_info: \" + str(response.status_code) + \" returned when sending a request to \\\"\" + url+\"\\\"\")\n\t\telse:\n\t\t\text = \"\"\n\t\t\tif (dataSet['file'].endswith('csv')):\n\t\t\t\text = \".csv\"\n\t\t\telif (dataSet['file'].endswith('txt')):\n\t\t\t\text = \".txt\"\n\t\t\telse:\n\t\t\t\traise Exception(\"Invalid file type that is not txt or csv\")\n\t\t\tcsvFile = open(\"output\" + ext,\"w+\")\n\t\t\tcsvFile.write(response.text)\n\t\t\tcsvFile.close()\n\t\t\tdf4 = pd.read_csv(\"output\" + ext)\n\t\t\t# Files output.csv/output.txt/output.pkl will persist\n\t\t\tdf4.to_pickle('output.pkl')\n\t\t\tdf4 = pd.read_pickle('output.pkl')\nelse:\n\tconn = jaydebeapi.connect(dataSource['driver_class'], [dataSource['URL'], dataSource['user'], dataSource['password']])\n\tif (len(dataSet['schema'].strip()) != 0):\n\t\tdf4 = pd.read_sql('select * from ' + dataSet['schema'] + '.' + dataSet['table'], con=conn)\n\telse:\n\t\tdf4 = pd.read_sql('select * from ' + dataSet['table'], con=conn)\ndf4.head()\n\n", 
            "outputs": [], 
            "cell_type": "code"
        }, 
        {
            "execution_count": null, 
            "metadata": {}, 
            "source": "# For Scala model training and deployment notebook:  Automatically generated code for inserting Spark dataframe for \"cust_summary_notebook_training\" dataset\nimport com.ibm.analytics.dsxCoreUtils._\nimport scala.util.{Try, Success, Failure}\nimport java.io._\n// Add asset from remote connection\nval data = new DataUtil()\nval retTryDf = data.getRemoteDataSet(sc, \"cust_summary_notebook_training\")\nval df2 = retTryDf.get\ndf2.show(5)", 
            "outputs": [], 
            "cell_type": "code"
        }
    ]
}